'''
Utility to help "tail" AWS logs stored in S3 generated by S3 bucket
logging or ELB logging.
'''
import os
import logging
from threading import Thread
from Queue import Queue
from datetime import datetime, timedelta
from tempfile import NamedTemporaryFile
from hashlib import sha256
from boto import connect_s3
from boto.s3 import connect_to_region

class Cache(object):
    readers = []

    class Reader(object):
        def __init__(self, reader, cache_pn):
            self.closed = False
            self._logger = logging.getLogger('s3tail.cache.reader')
            self._reader = reader
            self._cache_pn = cache_pn
            # write to a tempfile in case of failure; move into place when writing is complete
            head, tail = os.path.split(cache_pn)
            self._tempfile = NamedTemporaryFile(dir=head, prefix=tail)
            self._writer = BackgroundWriter(self._tempfile, self._move_into_place)
            self._writer.start()
            Cache.readers.append(self)

        def read(self, size=-1):
            data = self._reader.read(size)
            self._writer.write(data)
            return data

        def close(self):
            self._reader.close()
            self._writer.mark_done() # allow writer to finish async, not requiring caller to wait
            self.closed = True

        def cleanup(self):
            self._writer.join()

        def _move_into_place(self, _):
            self._tempfile.delete = False # prevent removal on close
            self._tempfile.close()
            os.rename(self._tempfile.name, self._cache_pn)
            Cache.readers.remove(self)
            self._logger.debug('Placed: %s', self._cache_pn)

    def __init__(self, path, hours):
        self._logger = logging.getLogger('s3tail.cache')
        self.path = path
        if not os.path.isdir(path):
            os.mkdir(path)
            # create shard buckets for sha hexstring names
            chars = range(ord('0'), ord('9')+1) + range(ord('a'), ord('f')+1)
            for i in chars:
                for j in chars:
                    os.mkdir(os.path.join(path, chr(i)+chr(j)))
        else:
            cleaner = OldFileCleaner(path, hours)
            cleaner.start()

    def open(self, name, reader):
        safe_name = sha256(name).hexdigest()
        cache_pn = os.path.join(self.path, safe_name[0:2], safe_name)
        if os.path.exists(cache_pn):
            if self._logger.isEnabledFor(logging.DEBUG):
                self._logger.debug('Found %s in cache: %s', name, cache_pn)
            else:
                self._logger.info('Found %s in cache', name)
            reader.close()
            return open(cache_pn)
        return Cache.Reader(reader, cache_pn)

    def cleanup(self):
        for reader in Cache.readers:
            reader.cleanup()

class S3Tail(object):
    BUFFER_SIZE = 1 * (1024*1024) # MiB

    def __init__(self, bucket_name, prefix, line_handler,
                 key_handler=None, bookmark=None, region=None, hours=24):
        self._cache = Cache(os.path.join(os.path.expanduser('~'), '.s3tailcache'), hours)
        if region:
            self._conn = connect_to_region(region)
        else:
            self._conn = connect_s3()
        self._bucket = self._conn.get_bucket(bucket_name)
        self._prefix = prefix
        self._line_handler = line_handler
        self._key_handler = key_handler
        if bookmark:
            self._bookmark_key, self._bookmark_line_num = bookmark.split(':')
            if len(self._bookmark_key) == 0:
                self._bookmark_key = None
            else:
                self._bookmark_line_num = int(self._bookmark_line_num)
        else:
            self._bookmark_key = None
            self._bookmark_line_num = 0
        self._marker = None
        self._buffer = None
        self._line_num = None

    def get_bookmark(self):
        if self._marker:
            return self._marker + ':' + str(self._line_num)
        if self._line_num:
            return ':' + str(self._line_num)

    def watch(self):
        for key in self._bucket.list(prefix=self._prefix, marker=self._bookmark_key):
            self._bookmark_key = None
            if self._key_handler:
                result = self._key_handler(key.name)
                if not result:
                    continue
            result = self._read(key)
            if result is not None:
                return result
            self._marker = key.name # marker always has to be _previous_ entry, not current

    def cleanup(self):
        self._cache.cleanup()

    ######################################################################
    # private

    def _read(self, key):
        self._buffer = ''
        self._line_num = 0
        key.open()
        reader = self._cache.open(key.name, key)
        while not reader.closed:
            line = self._next_line(reader)
            self._line_num += 1
            if self._line_num < self._bookmark_line_num:
                continue
            self._bookmark_line_num = 0
            result = self._line_handler(self._line_num, line)
            if result is not None:
                return result
        self._bookmark_line_num = 0 # safety in case bookmark count was larger than actual lines

    def _next_line(self, reader):
        i = None
        for _ in range(0, 3): # try reading up to three times the buffer size
            i = self._buffer.find("\n")
            if i > -1:
                break
            more_data = reader.read(S3Tail.BUFFER_SIZE)
            if len(more_data) > 0:
                self._buffer += more_data
            else:
                reader.close()
                i = len(self._buffer) + 1 # use remaining info in buffer
                break
        line = self._buffer[0:i]
        self._buffer = self._buffer[i+1:]
        return line

class BackgroundWriter(Thread):
    def __init__(self, writer, done_callback=None):
        '''Wraps a writer I/O object with background write calls.

        Optionally, will call the done_callback just before the thread stops (to allow caller to
        close/operate on the writer)
        '''
        super(BackgroundWriter, self).__init__()
        self._done = False
        self._done_callback = done_callback
        self._queue = Queue()
        self._writer = writer

    def write(self, data):
        self._queue.put(data)

    def mark_done(self):
        if not self._done:
            self._done = True
            self._queue.put(True)

    def join(self, timeout=None):
        self.mark_done()
        self._queue.join()
        super(BackgroundWriter, self).join(timeout)

    def run(self):
        while True:
            data = self._queue.get()
            if data is True:
                self._queue.task_done()
                if self._done_callback:
                    self._done_callback(self._writer)
                return
            self._writer.write(data)
            self._queue.task_done()

class OldFileCleaner(Thread):
    def __init__(self, path, hours):
        super(OldFileCleaner, self).__init__()
        self._logger = logging.getLogger('s3tail.old_file_cleaner')
        self._path = path
        self._hours = hours

    def run(self):
        count = 0
        for dirpath, _, filenames in os.walk(self._path):
            for ent in filenames:
                curpath = os.path.join(dirpath, ent)
                file_modified = datetime.fromtimestamp(os.path.getatime(curpath))
                if datetime.now() - file_modified > timedelta(hours=self._hours):
                    self._logger.debug('Removing %s', curpath)
                    os.remove(curpath)
                    count += 1
        if count > 0:
            self._logger.info('Cleaned up %d files', count)
