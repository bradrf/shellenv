'''
Utility to help "tail" AWS logs stored in S3 generated by S3 bucket
logging or ELB logging.
'''
import os
from threading import Thread
from Queue import Queue
from datetime import datetime, timedelta
from tempfile import NamedTemporaryFile
from hashlib import sha256
from boto import connect_s3
from boto.s3 import connect_to_region

class BackgroundWriter(Thread):
    def __init__(self, writer):
        super(BackgroundWriter, self).__init__()
        self._done = False
        self._queue = Queue()
        self._writer = writer

    def write(self, data):
        self._queue.put(data)

    def join(self):
        self._queue.put(True)
        self._queue.join()
        super(BackgroundWriter, self).join()

    def run(self):
        while True:
            data = self._queue.get()
            if data is True:
                self._queue.task_done()
                return
            self._writer.write(data)
            self._queue.task_done()

class OldFileCleaner(Thread):
    def __init__(self, path, hours):
        super(OldFileCleaner, self).__init__()
        self._path = path
        self._hours = hours

    def run(self):
        for dirpath, _, filenames in os.walk(self._path):
            for ent in filenames:
                curpath = os.path.join(dirpath, ent)
                file_modified = datetime.fromtimestamp(os.path.getmtime(curpath))
                if datetime.now() - file_modified > timedelta(hours=self._hours):
                    os.remove(curpath)

class Cache(object):
    class Reader(object):
        def __init__(self, reader, cache_pn):
            self.closed = False
            self._reader = reader
            self._cache_pn = cache_pn
            # write to a tempfile in case of failure; move into place on close
            head, tail = os.path.split(cache_pn)
            self._tempfile = NamedTemporaryFile(dir=head, prefix=tail)
            self._writer = BackgroundWriter(self._tempfile)
            self._writer.start()

        def read(self, size=-1):
            data = self._reader.read(size)
            self._writer.write(data)
            return data

        def close(self):
            self._reader.close()
            self._writer.join()
            self._tempfile.delete = False # prevent removal on close
            self._tempfile.close()
            os.rename(self._tempfile.name, self._cache_pn)
            self.closed = True

    def __init__(self, path, hours):
        self.path = path
        if not os.path.isdir(path):
            os.mkdir(path)
            # create shard subdirs for sha hexstring buckets
            chars = range(ord('0'), ord('9')+1) + range(ord('a'), ord('f')+1)
            for i in chars:
                for j in chars:
                    os.mkdir(os.path.join(path, chr(i)+chr(j)))
        else:
            cleaner = OldFileCleaner(path, hours)
            cleaner.start()

    def open(self, name, reader):
        safe_name = sha256(name).hexdigest()
        cache_pn = os.path.join(self.path, safe_name[0:2], safe_name)
        if os.path.exists(cache_pn):
            reader.close()
            return open(cache_pn)
        return Cache.Reader(reader, cache_pn)

class S3Tail(object):
    BUFFER_SIZE = 1 * (1024*1024) # MiB

    # TODO: cache files locally for some amount of time (24 hours?)

    # TODO: support "globbing" to iterate mutliple sources fields
    # e.g. s3://unitycloud-collab-logs/production/s3/collab-production-s3-access-2016-07-23-*
    #      maybe only numerical globs assumeing times? somethign esle?

    def __init__(self, bucket_name, prefix, line_handler,
                 key_handler=None, bookmark=None, region=None, hours=24):
        self._cache = Cache(os.path.join(os.path.expanduser('~'), '.s3tailcache'), hours)
        if region:
            self._conn = connect_to_region(region)
        else:
            self._conn = connect_s3()
        self._bucket = self._conn.get_bucket(bucket_name)
        self._prefix = prefix
        self._line_handler = line_handler
        self._key_handler = key_handler
        if bookmark:
            self._bookmark_key, self._bookmark_line_num = bookmark.split(':')
            if len(self._bookmark_key) == 0:
                self._bookmark_key = None
        else:
            self._bookmark_key = None
            self._bookmark_line_num = 0
        self._marker = None
        self._buffer = None
        self._line_num = None

    def get_bookmark(self):
        if self._marker:
            return self._marker + ':' + str(self._line_num)
        if self._line_num:
            return ':' + str(self._line_num)

    def watch(self):
        # TODO: see about ordering response in reverse? should try to get most recent log in bucket?
        for key in self._bucket.list(prefix=self._prefix, marker=self._bookmark_key):
            self._bookmark_key = None
            if self._key_handler:
                result = self._key_handler(key.name)
                if not result:
                    continue
            result = self._read(key)
            if result is not None:
                return result
            self._marker = key.name

    ######################################################################
    # private

    def _read(self, key):
        self._buffer = ''
        self._line_num = 0
        key.open()
        reader = self._cache.open(key.name, key)
        while not reader.closed:
            line = self._next_line(reader)
            self._line_num += 1
            if self._line_num < self._bookmark_line_num:
                continue
            self._bookmark_line_num = 0
            result = self._line_handler(self._line_num, line)
            if result is not None:
                return result
        self._bookmark_line_num = 0 # safety in case bookmark count was larger than actual lines

    def _next_line(self, reader):
        i = None
        for _ in range(0, 3): # try reading up to three times the buffer size
            i = self._buffer.find("\n")
            if i > -1:
                break
            more_data = reader.read(S3Tail.BUFFER_SIZE)
            if len(more_data) > 0:
                self._buffer += more_data
            else:
                reader.close()
                i = len(self._buffer) + 1 # use remaining info in buffer
                break
        line = self._buffer[0:i]
        self._buffer = self._buffer[i+1:]
        return line
