#!/usr/bin/env python

import sys
import logging
import signal
import re
import threading
import queue

# FIXME:
# * requires REGION in non-US?

# TODO:
# * consider way to run in pipeline feeding jobs from input, e.g. downloading files
#   > grep file* files | s3workers ... download ...
# * package w/ required of "boto" and "future" (needed for 2-3 compat queue import)
# * add better select/accumulator examples in help (need to use RawDescriptionHelpFormatter)

from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from time import sleep
from boto import connect_s3
from boto.s3 import regions, connect_to_region

shards = [chr(i) for i in range(ord('a'),ord('z')+1)] + [chr(i) for i in range(ord('0'),ord('9')+1)]
log_levels = ['debug-all', 'debug', 'info', 'warning', 'error', 'critical']

parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)
parser.add_argument('--log-level', choices=log_levels,
                    help='set logging level', default='info')
parser.add_argument('--region', choices=[r.name for r in regions()],
                    help='AWS region to use when connecting')
parser.add_argument('--select')
parser.add_argument('--accumulator', default='0')
parser.add_argument('--reduce')
parser.add_argument('--concurrency', type=int, default=len(shards))
parser.add_argument('command', choices=['list', 'delete'])
parser.add_argument('s3_uri')
parser.epilog = '''
Example: %(prog)s list s3://mybucket/ --select 'md5 == "d41d8cd98f00b204e9800998ecf8427e"'
'''
opts = parser.parse_args()

log_level_index = log_levels.index(opts.log_level)
log_kwargs = {
    'level': getattr(logging, log_levels[log_level_index + 1].upper()),
    'format': '[%(asctime)s #%(process)d %(threadName)s] %(levelname)-8s %(name)-12s %(message)s',
    'datefmt': '%Y-%m-%dT%H:%M:%S%z',
}

logging.basicConfig(**log_kwargs)

_logger = logging.getLogger(parser.prog)
if log_level_index > 0:
    _logger.setLevel(getattr(logging, log_levels[log_level_index].upper()))

class Job(object):
    def __init__(self):
        self._state = 'ready'
        self._stop_requested = threading.Event()

    def run(self, *args, **kwargs):
        try:
            if not self._stop_requested.isSet():
                self._state = 'running'
                self._runner(*args, **kwargs)
        finally:
            self._state = 'stopped'

    def stop(self):
        self._stop_requested.set()

    def __str__(self, details = ''):
        return '%s(%s) is %s' % (self.__class__.__name__, details, self._state)

    def _runner(self):
        pass

class S3ListJob(Job):
    def __init__(self, bucket, prefix, selector, key_handler, progress):
        super(self.__class__, self).__init__()
        self._bucket = bucket
        self._prefix = prefix
        self._selector = selector
        self._key_handler = key_handler
        self._progress = progress

    def __str__(self):
        return super(self.__class__, self).__str__(self._bucket.name + '/' + self._prefix)

    def _runner(self):
        for key in self._bucket.list(prefix=self._prefix):
            if self._stop_requested.isSet():
                break
            self._progress()
            if not key.md5:
                key.md5 = key.etag[1:-1] # GROSS. HACK. Likely break if multipart-uploaded...
            if self._is_selected(key):
                self._key_handler(key)

    def _is_selected(self, key):
        if not self._selector:
            return True
        size = key.size
        name = key.name
        md5 = key.md5
        return eval(self._selector)

class Worker(threading.Thread):
    _all_jobs_submitted = threading.Event()

    @classmethod
    def all_jobs_submitted(self):
        self._all_jobs_submitted.set()

    def __init__(self, work):
        super(self.__class__, self).__init__()
        self._work = work
        self._current_lock = threading.Lock()
        self._current_job = None
        self._stop_requested = threading.Event()

    def __str__(self):
        statestr = 'alive' if self.is_alive() else 'dead'
        jobstr = ' current=' + str(self._current_job) if self._current_job else ''
        return '%s(%s%s)' % (self.__class__.__name__, statestr, jobstr)

    def run(self):
        while True:
            if self._stop_requested.isSet():
                break
            if Worker._all_jobs_submitted.isSet() and self._work.empty():
                break
            try:
                with self._current_lock:
                    self._current_job = self._work.get(timeout=0.1)
            except queue.Empty:
                continue
            try:
                _logger.debug('starting: %s', self._current_job)
                self._current_job.run()
            finally:
                self._work.task_done()
                with self._current_lock:
                    self._current_job = None

    def stop(self):
        self._stop_requested.set()
        with self._current_lock:
            if self._current_job:
                self._current_job.stop()

class SimpleProgress(object):
    def __init__(self):
        self._lock = threading.RLock()

    def report(self, msg, *args):
        with self._lock:
            sys.stdout.write("\r" + (msg % args))
            sys.stdout.flush()

    def write(self, msg, *args):
        with self._lock:
            sys.stdout.write("\r" + (msg % args) + "\n")
            sys.stdout.flush()

    def finish(self):
        sys.stdout.write("\n")
        sys.stdout.flush()

class S3KeyProgress(SimpleProgress):
    def __init__(self):
        super(self.__class__, self).__init__()
        self._counter = 0
        self._selected = 0

    def report(self):
        with self._lock:
            self._counter += 1
            super(self.__class__, self).report('Selected %d of %d keys',
                                               self._selected, self._counter)
    def write(self, msg, *args):
        with self._lock:
            self._selected += 1
            super(self.__class__, self).write(msg, *args)

progress = S3KeyProgress()

accumulator = None
if opts.reduce:
    accumulator = eval(opts.accumulator)
    accumulator_lock = threading.Lock()
    exec('def reducer(accumulator, name, size, last_modified): ' +
         opts.reduce +
         '; return accumulator')
    def key_dumper(key):
        global accumulator
        with accumulator_lock:
            accumulator = reducer(accumulator, key.name, key.size, key.last_modified)
            progress.write('%s %10d %s %s => %s',
                           key.last_modified, key.size, key.md5, key.name, accumulator)
else:
    def key_dumper(key):
        progress.write('%s %10d %s %s', key.last_modified, key.size, key.md5, key.name)

def key_deleter(key):
    progress.write('DELETING: %s %10d %s %s', key.last_modified, key.size, key.md5, key.name)
    key.delete()

work = queue.Queue(opts.concurrency * 3)
workers = []

for i in range(opts.concurrency):
    worker = Worker(work)
    worker.start()
    workers.append(worker)

stopped = threading.Event()
def stop_work(*args):
    stopped.set()
    _logger.info('Stopping! work_item_count=%d', work.qsize())
    for worker in workers:
        if worker.is_alive():
            _logger.debug(worker)
            worker.stop()

signal.signal(signal.SIGINT, stop_work)
signal.signal(signal.SIGTERM, stop_work)
signal.signal(signal.SIGPIPE, stop_work)

s3_uri = re.sub(r'^(s3:)?/+', '', opts.s3_uri)
bucket_name, prefix = s3_uri.split('/', 1)

selector = compile(opts.select, '<select>', 'eval') if opts.select else None
handler = key_deleter if opts.command == 'delete' else key_dumper

conn = connect_to_region(opts.region) if opts.region else connect_s3()
bucket = conn.get_bucket(bucket_name)

_logger.info('Preparing %d jobs for %d workers', len(shards) * len(shards), len(workers))

# break up jobs into 2 char prefix elements
for shard1 in shards:
    if stopped.isSet():
        break
    for shard2 in shards:
        if stopped.isSet():
            break
        prefix_shard = prefix + shard1 + shard2
        job = S3ListJob(bucket, prefix_shard, selector, handler, progress.report)
        _logger.debug('Submitting %s', job)
        work.put(job)

Worker.all_jobs_submitted()
_logger.debug('All jobs submitted. Waiting on %d to complete.', work.qsize())

while threading.active_count() > 1:
    sleep(0.1)

for worker in workers:
    worker.join(1)

progress.finish()

if accumulator:
    print 'accumulator: ' + str(accumulator)
