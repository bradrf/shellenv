#  -*- mode: shell-script -*-

# TODO: look into replaceing ec2_wait_for w/ aws ec2 wait cmd

if ! declare -f ihave >/dev/null 2>&1; then
    function ihave() {
        \which "$@" >/dev/null 2>&1 || declare -f "$@" >/dev/null 2>&1
    }
fi

aws_fn="${HOME}/creds/aws-${USER}.conf"
if [ -f "$aws_fn" ]; then
    export AWS_CONFIG_FILE="$aws_fn"
    export BOTO_CONFIG="$aws_fn"
    mkdir -p "${HOME}/.aws"
    ln -sf "$aws_fn" "${HOME}/.aws/credentials"
fi
unset aws_fn

$INTERACTIVE && ihave aws_completer && complete -C aws_completer aws

alias s3='aws s3'
alias ec2='aws ec2'
alias elb='aws elb'
alias iam='aws iam'
alias aws_whoami='aws sts get-caller-identity'
# if only "Output" then it's the wrong region!, should create a func for this?
alias ec2logs='ec2 --output text get-console-output --instance-id'
alias ec2env='env | grep "^EC2_"'

if ihave ec2metadata; then
    # TODO: add helper to access all metadata
    # > curl http://169.254.169.254/latest/meta-data/iam/security-credentials/collab/

    # convert all metadata into ec2 env vars
    eval `ec2metadata | awk 'NR==1,/^user-data:/{\
if($1 != "user-data:"){\
gsub("-","_",$1);gsub(":","",$1);k="EC2_"toupper($1);$1="";sub(/^[[:space:]]*/,"",$0);\
print "export "k"=\""$0"\""}}'`

    function export_instance_creds()
    {
        local url='http://169.254.169.254/latest/meta-data/iam/security-credentials/'
        if [ $# -ne 1 ]; then
            echo 'export_instance_creds <role>' >&2
            curl -sS "$url"
            echo
            return 1
        fi
        eval `curl -sS "${url}$1" | awk '
/AccessKeyId/{gsub(",","",$3);print "id=" $3}
/SecretAccessKey/{gsub(",","",$3);print "key=" $3}'`
        export AWS_ACCESS_KEY_ID="$id"
        export AWS_SECRET_ACCESS_KEY="$key"
    }
fi

function to_ip()
{
    echo "$1" | \
        sed -n 's/^ *ip-\([[:digit:]]*\)-\([[:digit:]]*\)-\([[:digit:]]*\)-\([[:digit:]]*\).*$/\1.\2.\3.\4/p'
}

function awsregion()
{
    export AWS_REGION="$1"
    [ -n "$AWS_REGION" ] && export AWS_DEFAULT_REGION="$AWS_REGION" || unset AWS_DEFAULT_REGION
}

function awsenv()
{
    local v region
    if [ -n "$EC2_AVAILABILITY_ZONE" ]; then
        # set region based on instance info
        region="${EC2_AVAILABILITY_ZONE%?}" # strip availibility character at end
    fi
    if [ -n "$1" ]; then
        for v in FLASH NODE_ENV AWS_PROFILE AWS_REGION; do unset $v; done
        case "$1" in
            dev*)
                export AWS_ENV="development-${USER}"
                region=${region:-us-west-2}
                ;;
            andy*)
                export AWS_ENV="andy"
                FLASH="{ $AWS_ENV } "
                export NODE_ENV="$AWS_ENV"
                region=${region:-us-west-1}
                ;;
            stag*)
                export AWS_ENV='staging'
                FLASH="{ $AWS_ENV } "
                export NODE_ENV="$AWS_ENV"
                region=${region:-us-west-1}
                ;;
            gw)
                export AWS_ENV='gw'
                FLASH="{ $AWS_ENV } "
                export AWS_PROFILE=$AWS_ENV
                ;;
            prod*)
                export AWS_ENV='production'
                FLASH="*** PRODUCTION *** "
                ;;
            *)
                echo "Unknown AWS environment value: $1" >&2
                return 1
        esac
        [ -n "$AWS_ENV" ] && AWS_ENV_PREFIX="${AWS_ENV}-" || unset AWS_ENV_PREFIX
        awsregion "$region"
        [ -n "$AWS_PROFILE" ] && export AWS_DEFAULT_PROFILE="$AWS_PROFILE" || unset AWS_DEFAULT_PROFILE
    fi

    echo "${AWS_ENV}:"
    env | grep -E '^(AWS|BOTO)' | sort | sed 's/^/  /'
}

function ec2din()
{
    if [ $# -eq 2 ]; then
        aws --region "$1" ec2 --output text describe-instances --instance-id "$2"
        return
    fi
    saws -t instances "$@"| sort | column -t
}

# get console output for an instance ID
alias ec2conout='aws ec2 get-console-output --output text --query Output --instance-id'

# python app in bin, now, but this is still handy for remote instances
if ! ihave elbls; then
    function elbls()
    {
        aws elb describe-load-balancers --output text \
            --query 'LoadBalancerDescriptions[*].[LoadBalancerName, DNSName, Instances]' | \
            awk '{if (match($1,/^i-/)) {ins = ins "\t" $1;} else {if (node) {print node ins;} \
node = $0; ins = ""}} END {if (node) {print node ins;}}' | \
            sort | column -t
    }
fi

if ! ihave elblsf; then
    function elblsf()
    {
        local info
        aws elb describe-load-balancers --output text \
            --query 'LoadBalancerDescriptions[*].[LoadBalancerName, DNSName]' | \
            while read -r info; do
                name=$(echo "$info" | tee /dev/tty | awk '{print $1}')
                aws elb describe-tags --load-balancer-names "$name" --output text \
                    --query 'TagDescriptions[*].Tags[*]' | prefix '  TAG: '
            done
    }
fi

function elbreg()
{
    local action iid
    if [ "$1" = '-u' ]; then
        action='deregister-instances-from-load-balancer'; shift
    else
        action='register-instances-with-load-balancer'
    fi
    if [ $# -eq 2 ]; then
        iid="$1"; shift
    else
        iid="$EC2_INSTANCE_ID"
    fi
    if [ $# -ne 1 -o -z "$iid" ]; then
        echo 'elbreg [-u] [<instace_id>] <elb_name>' >&2
        return 1
    fi
    aws --output text elb $action --load-balancer-name "$1" --instances "$iid"
}

function awsdnsdump()
{
    saws -t names "$@" | sort | column -t
}

function awslogs()
{
    local group cmd stream start
    if [ "$1" = '-s' ]; then
        # avoid missing events and only look up to this many hours ago...
        start=$(expr `date -d "$1 hours ago" +%s` \* 1000)
        echo "Starting time: $start"
        start="--start-time $start"
        shift; shift;
    fi
    if [ $# -lt 1 ]; then
        ( echo 'usage: awslogs [-s <start_hours_ago>] <group_name> [<stream> | latest]';
          aws logs describe-log-groups --output text --query 'logGroups[*].[storedBytes,logGroupName]' ) >&2
        return 1
    fi
    group="$1"
    cmd="aws logs describe-log-streams --output text --log-group-name $group \
--query logStreams[*].[creationTime,logStreamName]"
    echo $cmd
    if [ -z "$2" ]; then
        $cmd | sort | awk '{ print strftime("%Y-%m-%dT%H:%M:%S%z", $1/1000) " " $2}'
        return
    fi
    if [ "$2" = 'latest' ]; then
        stream=`$cmd | sort | awk 'END { print $2 }'`
    else
        stream="$2"
    fi
    aws logs get-log-events --log-group-name $group --log-stream-name $stream --output text \
        $start --query 'events[*].[timestamp,message]' | \
        awk '/^[[:digit:]]+/ { print strftime("%Y-%m-%dT%H:%M:%S%z",$1/1000) " " $0 } !/^[[:digit:]]+/ { print $0 }'
}

function sqsq()
{
    aws sqs get-queue-url --queue-name "${AWS_ENV_PREFIX}$1" --output text
}

function sqsls()
{
    local q
    local attrs='ApproximateNumberOfMessages ApproximateNumberOfMessagesNotVisible ApproximateNumberOfMessagesDelayed'
    if [ -n "$1" ]; then
        q=`sqsq $1`
        echo "$q"
        aws sqs get-queue-attributes --queue-url "$q" --query 'Attributes' --attribute-names $attrs
    else
        local q s
        for q in `aws sqs list-queues --output text --query 'QueueUrls[*]'`; do
            echo "$q" | grep -qF "${AWS_ENV_PREFIX}" || continue
            s="$(basename "$q")"
            (aws sqs get-queue-attributes --queue-url "$q" --query 'Attributes' --attribute-names $attrs | \
                    awk -v q="${s}" '!/: "0"|^[{}]$/ {print q,$0}' &) | sort
            # above will show only non-zero counts
        done
    fi
}

function sqspeek()
{
    local count
    [ -n "$2" ] && count=$2 || count=1
    aws sqs receive-message --queue-url `sqsq $1` --output text --query 'Messages[*].[Body]' \
        --max-number-of-messages $count --visibility-timeout 1 | prettyjson
}

function sqspop()
{
    local url fn handle
    url=`sqsq $1`
    fn=`mktemp`
    aws sqs receive-message --queue-url "$url" --query 'Messages' | tee "$fn"
    handle=`cat "$fn" | awk '/ReceiptHandle/ {gsub(/[,"]/,""); print $2}'`
    aws sqs delete-message --queue-url "$url" --receipt-handle "$handle"
}

function sqspush()
{
    if [ $# -ne 2 ]; then
        echo 'usage: sqspush <queue_name> <body>' >&2
        return 1
    fi
    aws sqs send-message --queue-url `sqsq $1` --message-body "$2"
}

function sqspurge()
{
    if [ $# -ne 1 ]; then
        echo 'usage: sqspurge <queue_name>' >&2
        return 1
    fi
    aws sqs purge-queue --queue-url `sqsq $1`
}

function _s3parseargs()
{
    if [ "$1" = '--region' ]; then
        shift; _S3ARGS=(--region $1); shift
    fi

    if [ $# -ne 1 ]; then
        echo "usage: [--region <region>] <s3_key>" >&2
        return 1
    fi

    _S3KEY="${1#s3://}"
    _S3BUCKET="$(echo "$_S3KEY" | cut -d/ -f1)"
    _S3FN="${TMP:-/tmp}/$(echo "$_S3KEY" | tr / _)"
}

function s3get()
{
    _s3parseargs "$@" && (
        umask 0077
        aws s3 "${_S3ARGS[@]}" cp "s3://${_S3KEY}" "$_S3FN"
    )
}

function s3lscp()
{
    # TODO: pop last arg to use as a destination directory
    # local dst="${@: -1}"
    local dst='.'
    _s3parseargs "$@" || return $?
    local dn="$(dirname "${_S3KEY}")"
    aws s3 "${_S3ARGS[@]}" ls "s3://${_S3KEY}" | awk '{print $4}' | while read bn; do
        aws s3 "${_S3ARGS[@]}" cp "s3://${dn}/${bn}" "${dst}"
    done
}

# FIXME: this still allows the "downloaded" message from aws s3 cmd. probably need to use quiet
#        (i.e. it pollutes anything redirected, say a YAML file)
function s3cat()
{
    s3get "$@" && cat "$_S3FN" && \rm -f "$_S3FN"
}

# TODO: validate file if known extension (i.e. try parsing as json/yaml/etc)
ihave colordiff && _S3DIFF=colordiff || _S3DIFF=diff
function s3edit()
{
    _s3parseargs "$@" || return $?

    local rsp="`s3get "$@" 2>&1`"
    local rc=$?
    echo "$rsp"

    # create a new file if not found (404)
    echo "$rsp" | grep -qF ' (404) ' && touch "$_S3FN"

    test -f "$_S3FN" && cp "$_S3FN" "${_S3FN}.orig" && $EDITOR "$_S3FN"
    rc=$?
    if [ $rc -eq 0 ] && ! diff -q "${_S3FN}.orig" "$_S3FN"; then
        $_S3DIFF -u "${_S3FN}.orig" "$_S3FN"
        echo; printf "Save to ${_S3KEY} (y|N)? "; read
        [ "$REPLY" = 'y' ] && aws s3 "${_S3ARGS[@]}" cp "$_S3FN" "s3://${_S3KEY}"
    fi

    \rm -f "${_S3FN}"*

    return $rc
}

function s3props()
{
    local action
    local location
    local args=(--output text s3api)
    for action in location acl policy logging lifecycle; do
        echo "-- ${action} ------------------------------"
        if [[ $action == 'location' ]]; then
            location=$(aws "${args[@]}" get-bucket-location --bucket "$1" | tee /dev/tty)
            [[ -z "$location" ]] && return 1
            [[ "$location" == 'None' ]] || args=(--region "$location" "${args[@]}")
        else
            aws "${args[@]}" get-bucket-${action} --bucket "$1"
        fi
    done
}

# shows all the metadata info about an object
function s3head()
{
    _s3parseargs "$@" || return $?
    local skey="$(echo "${_S3KEY}" | cut -d/ -f2-)"
    aws "${_S3ARGS[@]}" s3api head-object --bucket "${_S3BUCKET}" --key "${skey}"
}

function s3ls()
{
    local OPTIND OPTARG OPTERR opt sorter key
    local args=()

    if [ "$1" = '--region' ]; then
        shift; args=(--region $1); shift
    fi

    while getopts hrsSt opt; do
        case $opt in
            h) args=("${args[@]}" --human-readable);;
            r) args=("${args[@]}" --recursive);;
            s) args=("${args[@]}" --summarize);;
            S) sorter='sort -k3';;
            t) sorter=sort;;
            \?) echo 'usage: s3ls [--region <region>] [hrsSt] [<key>]' >&2; return 1;;
        esac
    done

    shift $((OPTIND-1)) # move all remain args to first position
    key="${1#s3://}"

    if [ $# -lt 1 ]; then
        aws s3 ls $args s3://
    elif [ -n "$sorter" ]; then
        # this diverts all lines not starting with 20 (i.e. timestamps) to the end of the report
        # into stderr to avoid getting sorted
        local tf="$(mktemp)"
        aws s3 ls "${args[@]}" "s3://${key}" |
            awk '/^20/{print} !/^20/{print >> "'"${tf}"'"}' |
            $sorter
        cat "$tf"
        \rm -f "$tf"
    else
        aws s3 ls "${args[@]}" "s3://${key}"
    fi
}

function s3rm()
{
    _s3parseargs "$@" || return 1
    aws s3 "${_S3ARGS[@]}" rm --recursive "s3://${_S3KEY}"
}

function s3bucketmetrics()
{
    if [[ $# -lt 1 ]]; then
        echo 'usage: s3bucketmetrics <bucket> [<bucket>...]' >&2
        return 1
    fi
    local cargs bucket count bytes
    cargs=(
        cloudwatch get-metric-statistics
        --namespace AWS/S3 --statistics Average --period 300
        --output json --query Datapoints[0].Average
        --start-time "$(date -u -v-1d "$ISO8601_FMT")" --end-time "$(date -u "$ISO8601_FMT")"
    )
    for bucket in "$@"; do
        count=$(aws "${cargs[@]}" --metric-name NumberOfObjects \
                    --dimensions "Name=BucketName,Value=$bucket" \
                    Name=StorageType,Value=AllStorageTypes)
        bytes=$(aws "${cargs[@]}" --metric-name BucketSizeBytes \
                    --dimensions "Name=BucketName,Value=$bucket" \
                    Name=StorageType,Value=StandardStorage)
        echo "${bucket}: objects=${count} bytes=${bytes}"
    done
}

function iamgetreport()
{
    aws iam generate-credential-report || return $?
    local tf="$(mktemp)"
    local rc
    sleep 3 # TODO: only sleep if response is not "State": "COMPLETE"
    if aws iam get-credential-report --output json > "$tf"; then
        ruby -rjson -rtime -rbase64 -e 'j=JSON.load(File.open("'"$tf"'"));
d=Time.parse(j["GeneratedTime"]).utc;
f="#{d.strftime("%Y%m%d%H%M%S")}-credential-report.csv";
File.write(f,Base64.decode64(j["Content"]));
puts "IAM report saved to #{f}"'
        rc=$?
    else
        rc=$?
    fi
    \rm -f "$tf"
    return $rc
}

function sts_decode()
{
    aws sts decode-authorization-message --output text --query DecodedMessage --encoded-message "$1"
}

# TODO: add s3catlog to cat out files in a bucket that is used to host s3 access logs
#       (e.g. have it use today's date by default to start listing, and or some other mechanism to choose)

# TODO: add s3cp helper to iterate with multiple entries, like logs:
#       > for f in `cat l | awk '{print $4}'`; do s3 cp s3://unitycloud-collab-logs/production/s3/$f logs; don

# set default env to development
test -z "$AWS_ENV_PREFIX" && awsenv development >/dev/null
